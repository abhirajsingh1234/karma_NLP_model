{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "adaafab7-5d34-4f2e-9bc6-abad97279f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting chromadb\n",
      "  Using cached chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (2.10.4)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Using cached chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl.metadata (262 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Downloading fastapi-0.115.10-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.22.5 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (2.0.2)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Downloading posthog-3.18.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Using cached onnxruntime-1.20.1-cp310-cp310-win_amd64.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.21.0)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: tqdm>=4.65.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (1.70.0)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl.metadata (10 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tenacity>=8.2.3 (from chromadb)\n",
      "  Using cached tenacity-9.0.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Using cached mmh3-5.1.0-cp310-cp310-win_amd64.whl.metadata (16 kB)\n",
      "Collecting orjson>=3.9.12 (from chromadb)\n",
      "  Using cached orjson-3.10.15-cp310-cp310-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (8.5.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
      "  Downloading starlette-0.46.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.7.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (24.12.23)\n",
      "Requirement already satisfied: protobuf in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting opentelemetry-proto==1.30.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.30.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-instrumentation==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting opentelemetry-util-http==0.51b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: distro>=1.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tokenizers>=0.13.2->chromadb) (0.27.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached watchfiles-1.0.4-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached websockets-15.0-cp310-cp310-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.12.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from importlib-metadata>=4.6->build>=1.0.3->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\acer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Using cached chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
      "Using cached chroma_hnswlib-0.7.6-cp310-cp310-win_amd64.whl (150 kB)\n",
      "Downloading bcrypt-4.3.0-cp39-abi3-win_amd64.whl (152 kB)\n",
      "Downloading fastapi-0.115.10-py3-none-any.whl (94 kB)\n",
      "Using cached kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
      "Using cached mmh3-5.1.0-cp310-cp310-win_amd64.whl (41 kB)\n",
      "Using cached onnxruntime-1.20.1-cp310-cp310-win_amd64.whl (11.3 MB)\n",
      "Using cached opentelemetry_api-1.30.0-py3-none-any.whl (64 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.30.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_proto-1.30.0-py3-none-any.whl (55 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.51b0-py3-none-any.whl (12 kB)\n",
      "Using cached opentelemetry_instrumentation-0.51b0-py3-none-any.whl (30 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.51b0-py3-none-any.whl (16 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.51b0-py3-none-any.whl (177 kB)\n",
      "Using cached opentelemetry_util_http-0.51b0-py3-none-any.whl (7.3 kB)\n",
      "Using cached opentelemetry_sdk-1.30.0-py3-none-any.whl (118 kB)\n",
      "Using cached orjson-3.10.15-cp310-cp310-win_amd64.whl (133 kB)\n",
      "Downloading posthog-3.18.0-py2.py3-none-any.whl (76 kB)\n",
      "Using cached tenacity-9.0.0-py3-none-any.whl (28 kB)\n",
      "Downloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Using cached uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
      "Using cached importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached httptools-0.6.4-cp310-cp310-win_amd64.whl (88 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading starlette-0.46.0-py3-none-any.whl (71 kB)\n",
      "Using cached watchfiles-1.0.4-cp310-cp310-win_amd64.whl (284 kB)\n",
      "Using cached websockets-15.0-cp310-cp310-win_amd64.whl (176 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pypika, monotonic, durationpy, websockets, tenacity, shellingham, pyreadline3, orjson, opentelemetry-util-http, opentelemetry-proto, oauthlib, mmh3, importlib-resources, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, humanfriendly, typer, opentelemetry-semantic-conventions, kubernetes, fastapi, coloredlogs, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb\n",
      "Successfully installed backoff-2.2.1 bcrypt-4.3.0 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 deprecated-1.2.18 durationpy-0.9 fastapi-0.115.10 httptools-0.6.4 humanfriendly-10.0 importlib-resources-6.5.2 kubernetes-32.0.1 mmh3-5.1.0 monotonic-1.6 oauthlib-3.2.2 onnxruntime-1.20.1 opentelemetry-api-1.30.0 opentelemetry-exporter-otlp-proto-common-1.30.0 opentelemetry-exporter-otlp-proto-grpc-1.30.0 opentelemetry-instrumentation-0.51b0 opentelemetry-instrumentation-asgi-0.51b0 opentelemetry-instrumentation-fastapi-0.51b0 opentelemetry-proto-1.30.0 opentelemetry-sdk-1.30.0 opentelemetry-semantic-conventions-0.51b0 opentelemetry-util-http-0.51b0 orjson-3.10.15 posthog-3.18.0 pypika-0.48.9 pyreadline3-3.5.4 requests-oauthlib-2.0.0 shellingham-1.5.4 starlette-0.46.0 tenacity-9.0.0 typer-0.15.2 uvicorn-0.34.0 watchfiles-1.0.4 websockets-15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install chromadb\n",
    "\n",
    "import pandas\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer,T5ForConditionalGeneration,TrainingArguments, Trainer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5e7d1bc-98e5-4d3e-901a-c8010e8a541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pandas.read_csv('qa_pairs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd62b21-743d-48e6-a332-a76f054ba4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = pandas.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5d3e39-ed4f-4cf9-a87f-2b4ea6a01ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10112    Who may derive many benefits from worshipping ...\n",
       "11048    What should a king do even when flushed with v...\n",
       "5871     What type of yoga yields success in all undert...\n",
       "11599    What is the purpose of worshipping on the eigh...\n",
       "12464                    How many sons did the woman lose?\n",
       "9122     How should a seeker of salvation meditate upon...\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_pairs['question'].sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bab8144-2314-48a6-ba51-f1542ffd5853",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5b386d5d374d9ba2bedf82f6cf55eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19728 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9200b412e3c4cf1b777f9e535b1be4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4932 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_data(qa_pairs):\n",
    "    data=[]\n",
    "    for i,row in qa_pairs.iterrows():\n",
    "        question = row['question']\n",
    "        answer = row['answer']\n",
    "        context = row['verse_text']\n",
    "\n",
    "        input_text = f'question : {question} context : {context}'\n",
    "        output = answer\n",
    "\n",
    "        data.append({f\"input_text\" : input_text, 'target_text' : output })\n",
    "    return data\n",
    "    \n",
    "training_data = process_data(qa_pairs)\n",
    "\n",
    "\n",
    "# \n",
    "train_hf = Dataset.from_list(training_data)\n",
    "\n",
    "train_data, eval_data = train_test_split(training_data, test_size=0.2)\n",
    "\n",
    "train_df = pandas.DataFrame(train_data)\n",
    "eval_df = pandas.DataFrame(eval_data)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "model_name='t5-small'\n",
    "tokenizer= T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"input_text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(examples[\"target_text\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# ✅ Step 6: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39e6d17-5552-4e4e-98be-c4b0717b05a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No gradient for shared.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.0.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.1.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.2.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.3.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.4.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.1.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.1.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for encoder.block.5.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for encoder.final_layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.0.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.1.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.2.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.3.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.4.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.0.SelfAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.0.SelfAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.0.SelfAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.0.SelfAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.0.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.1.EncDecAttention.q.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.1.EncDecAttention.k.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.1.EncDecAttention.v.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.1.EncDecAttention.o.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.1.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.2.DenseReluDense.wi.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.2.DenseReluDense.wo.weight (Before Training)\n",
      "⚠️ No gradient for decoder.block.5.layer.2.layer_norm.weight (Before Training)\n",
      "⚠️ No gradient for decoder.final_layer_norm.weight (Before Training)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Ensure model is in training mode\n",
    "model.train()\n",
    "\n",
    "# ✅ Ensure all parameters require gradients\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True  \n",
    "\n",
    "# ✅ Fix potential gradient checkpointing issue\n",
    "model.config.use_cache = False  \n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad and param.grad is None:\n",
    "        print(f\"⚠️ No gradient for {name} (Before Training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed351c20-0d0c-4af2-ba7d-711eb74a9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ Step 7: Train the Model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ✅ Step 8: Save the Fine-Tuned Model\n",
    "model.save_pretrained(\"t5_karma_finetuned\")\n",
    "tokenizer.save_pretrained(\"t5_karma_finetuned\")\n",
    "\n",
    "print(\"🎉 Training Complete! Model saved as 't5_karma_finetuned'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2feb9194-9d8a-4df6-ac3b-af581d3bb669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 166 records...\n",
      "Inserted 73 records...\n",
      "✅ All data inserted successfully!\n"
     ]
    }
   ],
   "source": [
    "data=pandas.read_csv('Content_Storage_df.csv')\n",
    "embedding_df=pandas.DataFrame(data)\n",
    "embedding_df.head(1)\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_collection(\"karma_embeddings\")\n",
    "\n",
    "BATCH_SIZE = 166  # ChromaDB's max batch size\n",
    "\n",
    "def insert_data(df):\n",
    "    # Ensure embeddings are in the correct format (list of floats)\n",
    "    df[\"embedding_vector\"] = df[\"embedding_vector\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "    \n",
    "    # Convert verse_id to float format for uniqueness\n",
    "    df[\"verse_id\"] = df.apply(lambda row: float(f\"{row['chapter_id']}.{row['verse_id']}\"), axis=1)\n",
    "\n",
    "    # Insert data in batches\n",
    "    for i in range(0, len(df), BATCH_SIZE):\n",
    "        batch = df.iloc[i : i + BATCH_SIZE]\n",
    "\n",
    "        collection.add(\n",
    "            ids=batch[\"verse_id\"].astype(str).tolist(),      # Convert verse_id to string\n",
    "            documents=batch[\"verse_text\"].tolist(),          # List of verses\n",
    "            embeddings=batch[\"embedding_vector\"].tolist()    # List of embeddings\n",
    "        )\n",
    "        print(f\"Inserted {len(batch)} records...\")\n",
    "\n",
    "    print(\"✅ All data inserted successfully!\")\n",
    "\n",
    "# Call function to insert your data\n",
    "insert_data(embedding_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdf39de5-0f7f-4512-a893-5315be60eb24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he is the destroyer of kartavirya'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained('t5_karma_finetuned')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5_karma_finetuned')\n",
    "\n",
    "\n",
    "def retrieve_context(question, top_k=3):\n",
    "    embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
    "    question_embedding = embedding_fn([question])[0]\n",
    "\n",
    "    # Retrieve top-k matching documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[question_embedding],\n",
    "        n_results=top_k\n",
    "    )\n",
    "    if results[\"documents\"]:\n",
    "        # print(results[\"documents\"])\n",
    "        flat_documents = [doc for sublist in results[\"documents\"] for doc in sublist]\n",
    "        return \" \".join(flat_documents) if flat_documents else \"No relevant context found.\"\n",
    "    \n",
    "    return \"No relevant context found.\"\n",
    "\n",
    "def get_answer(question, context):\n",
    "    input_text = f\"question: {question}  context: {context}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "\n",
    "    output_ids = model.generate(input_ids, max_length=128)\n",
    "    answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01db40c0-f918-4d8d-b4ac-e08f384e6355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "user :  is sexually desired women attracted to other men?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'retrieve_context' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      2\u001b[0m     question \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser : \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mretrieve_context\u001b[49m(question)\n\u001b[0;32m      4\u001b[0m     answer \u001b[38;5;241m=\u001b[39m get_answer(question,context)\n\u001b[0;32m      5\u001b[0m     final_answer \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser : \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m answer\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retrieve_context' is not defined"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    question = input('user : ')\n",
    "    context = retrieve_context(question)\n",
    "    answer = get_answer(question,context)\n",
    "    final_answer ='user : '+ answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7518c0-5b70-40d3-9ee9-c9e83e959352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
